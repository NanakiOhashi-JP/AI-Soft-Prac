{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c392038-fcf2-44f5-b8b9-bed7f2923409",
   "metadata": {},
   "source": [
    "# 人工知能とソフトコンピューティング 第12回 RNN 演習(GitHubデータを使った予測・後半）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826c80c-20be-4bc9-8588-be2b06a6510f",
   "metadata": {},
   "source": [
    "## 分析対象となるデータの準備\n",
    "\n",
    "ここでは Apache HTTPD の開発履歴（1996/7/3～2023/12/23：27年間 33763 コミット）のデータから，あるコミットでのコード変更行数を予測するタスクを想定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da60dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path # ファイルパスの取り扱い\n",
    "\n",
    "# ベースパスの設定\n",
    "base_path = '.'\n",
    "# csvファイル名の設定\n",
    "csv_file_name = Path('apache_httpd.csv')\n",
    "# csv_file_name = Path('owncloud_android.csv') # 別のcsvファイルを使用する場合はこちらを有効化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38635a80-c6da-4d2b-a3d9-2b8bb0bf9472",
   "metadata": {},
   "source": [
    "### ステップ1\n",
    "必要となるライブラリをインポート（tensorflowの使える環境を選択する必要あり）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284eae3f-02d3-4562-84bd-20bb9c294770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # グラフ描画\n",
    "import pandas as pd # データの取り扱い\n",
    "from pandas import Series, DataFrame\n",
    "from sklearn.preprocessing import StandardScaler # データの正規化（平均・標準偏差）\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error #\n",
    "\n",
    "from datetime import datetime, timezone, date, timedelta # 時間関係（日付）の計算\n",
    "import numpy as np # 数値取扱い\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import pytorch_lightning as L\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c4e11-3113-4378-bd03-5d4d82b4e6c5",
   "metadata": {},
   "source": [
    "### ステップ2\n",
    "CSVファイルからデータを読み込み，その内容を確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c76095-ca9b-4d84-a65c-63c263187797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_df(df):\n",
    "    fig, ax = plt.subplots(figsize=(12,4))\n",
    "    ax.plot(df['timestamp'], df['changed'])\n",
    "    start_datetime = datetime(2021, 1, 1)\n",
    "    end_datetime = datetime(2023, 12, 31)\n",
    "    ax.axvspan(start_datetime, end_datetime, color=\"gray\", alpha=0.3) # 予測する部分を色付けして表示\n",
    "\n",
    "df = pd.read_csv(Path(base_path, \"12_data\", csv_file_name)) # csv ファイルからの読み込み\n",
    "df.loc[:, 'timestamp'] = [datetime.strptime(x, ' %a %b %d %H:%M:%S %Y %z') for x in df['timestamp']] # timestamp のフォーマットを変換（文字列からdatetimeへ）\n",
    "\n",
    "plot_df(df)\n",
    "df.head() # 先頭部分のデータを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350db989-326c-40e1-8832-7c60c7e3474c",
   "metadata": {},
   "source": [
    "### ステップ3\n",
    "* 読み込んだコミットデータの個数，変更行の最小値，変更行の最大値を確認\n",
    "* 読み込んだデータを正規化するための層を設定（平均値を0，標準偏差を1に正規化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9955c-6146-479d-9c66-2dc9b5db067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_base_data(df):\n",
    "    # 読み込んだ変更行数の値（データフレーム形式）を numpy形式の 2次元配列に変換\n",
    "    base_data = df.changed.values.reshape(-1, 1)\n",
    "    # データ個数，最小変更行数，最大変更行数の表示\n",
    "    print(\"コミット数 = {}\".format(len(base_data)))\n",
    "    print(\"コミット当たり最小の変更行数 = {} 行\".format(np.min(base_data)))\n",
    "    print(\"コミット当たり最大の変更行数 = {} 行\".format(np.max(base_data)))\n",
    "    return base_data\n",
    "\n",
    "def cleansing_by_changed(df, base_data):\n",
    "    # 突発的に多く変更される場合があるので，クレンジングを実施．\n",
    "    # 平均値＋3σ（正規分布であれば，99.7%から外れる）のデータを除外．\n",
    "    df_local = df.copy()\n",
    "    avg = np.average(base_data)\n",
    "    std = np.std(base_data)\n",
    "    df_local.drop(df_local[df_local[\"changed\"] > avg + 3 * std].index, inplace = True)\n",
    "    plot_df(df_local)\n",
    "    return df_local.reset_index()\n",
    "\n",
    "base_data = make_base_data(df)\n",
    "\n",
    "#print(\"クレンジング実施：\")\n",
    "#df = cleansing_by_changed(df, base_data)\n",
    "#base_data = make_base_data(df)\n",
    "\n",
    "scaler = StandardScaler() # scikit-learn のスケーラ\n",
    "scaler.fit(base_data) # 平均・標準偏差の計算\n",
    "base_data = scaler.transform(base_data) # 標準化変換"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab4297-598f-42b7-8eff-5986b81a50ae",
   "metadata": {},
   "source": [
    "## 訓練用データとテスト用データの作成\n",
    "一定の長さのコミットあたり変更行数の系列を作成\n",
    "* 直近 sequence_length 回をタイムステップとし，その回数のコミット当たり変更行数から次コミットの変更行数を推定する\n",
    "* sequence_length 回の変更行数の系列（入力データ）と sequence_length + 1 回目の変更行数（正解データ）を作成\n",
    "* 訓練用データはプロジェクト開始～2020年，テスト用データは 2021年以降とする\n",
    "    * ただし，訓練用データの開始年は，変更できるようにする．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1f4d5d-b993-4bb6-b3c0-0ba153d4fb19",
   "metadata": {},
   "source": [
    "### ステップ4\n",
    "系列長（sequence_length）とバッチサイズ（batch_size）を設定\n",
    "\n",
    "* 系列長は RNN に入力する温度系列の長さ（タイムステップ）\n",
    "* バッチサイズは RNN が重みを計算する際（最適化アルゴリズム）に与えるミニバッチサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd54c90-6408-47e4-b2f6-186c19880f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100 # 遡って何コミット分の変更行数データから次のコミットの変更行数を予測するか"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e4ac7-baeb-42ca-b03b-b0363cde5c3b",
   "metadata": {},
   "source": [
    "### ステップ5\n",
    "訓練用データとテスト用データを分割\n",
    "* 訓練用データは 開始年（1996が最初）/1/1 ～ 2020/12/31 の最大25年間 (クレンジングしていない場合は 32406件)\n",
    "* テスト用データは 2021/1/1 以降の(クレンジングしていない場合は) 1357件\n",
    "* 過去25年分のコミットデータを用いて RNNを訓練して，2021 以降約3年間のコミット当たり変更行数を予測\n",
    "* 実際の予測では，sequence_length 回分のデータを入力して次のコミットの変更行数を予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b740e-adc9-4d22-8a49-ba0ab33b7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練用データ start_year/1/1 ～ 2020/12/31\n",
    "start_year = 1996\n",
    "#start_year = 2016\n",
    "training_data = base_data[ df.index[df[\"timestamp\"] >= datetime(start_year, 1, 1, 0, 0, 0, 0, timezone.utc)].tolist()[0]:df.index[df[\"timestamp\"] >= datetime(2021, 1, 1, 0, 0, 0, 0, timezone.utc)].tolist()[0]]\n",
    "# テスト用データ 2021/1/1 以降\n",
    "test_data = base_data[df.index[df[\"timestamp\"] >= datetime(2021, 1, 1, 0, 0, 0, 0, timezone.utc)].tolist()[0] :]\n",
    "\n",
    "# 分割したデータの形式を確認\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4858757-30cc-4643-88c3-bed7e323ac59",
   "metadata": {},
   "source": [
    "### ステップ6\n",
    "* 直近 sequence_length 回分の連続系列を作り出す関数（generate_dataset）を定義\n",
    "* 定義した関数を訓練用データ，テスト用データに適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb407a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(data, length): # length日分の連続系列を作り出す\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - length):\n",
    "        p = i + length\n",
    "        X.append(data[i : p])\n",
    "        y.append(data[p])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# *_seqs は2次元配列：長さ sequence_length の温度値からなる系列が並んだもの\n",
    "# *_targets は1次元配列：予測目標である温度値が並んだもの\n",
    "training_seqs, training_targets = generate_dataset(training_data, sequence_length) # 訓練用の系列と正解値\n",
    "test_seqs, test_targets = generate_dataset(test_data, sequence_length) # テスト用の系列と正解値\n",
    "# RNNへの入力用に3階のテンソルの変換（データ個数，シーケンス長，データ次元数）\n",
    "training_seqs = np.reshape(training_seqs, (-1, sequence_length, 1))\n",
    "test_seqs = np.reshape(test_seqs, (-1, sequence_length, 1))\n",
    "# 具体的なデータやその形式を確認したい場合は次の4行のコメントを外す\n",
    "# print(training_seqs)\n",
    "# print(training_seqs.shape)\n",
    "# print(training_targets)\n",
    "# print(training_targets.shape)\n",
    "\n",
    "# NumPy配列をPyTorchのテンソルに変換\n",
    "training_seqs = torch.Tensor(training_seqs)\n",
    "training_targets = torch.Tensor(training_targets)\n",
    "test_seqs = torch.Tensor(test_seqs)\n",
    "test_targets = torch.Tensor(test_targets)\n",
    "\n",
    "# TensorDataset に変換\n",
    "# 検証用データを訓練用データの末尾から20%分抽出\n",
    "val_size = int(len(training_seqs) * 0.2)\n",
    "train_size = len(training_seqs) - val_size\n",
    "train_dataset = TensorDataset(training_seqs[:train_size], training_targets[:train_size])\n",
    "val_dataset = TensorDataset(training_seqs[train_size:], training_targets[train_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651111dc-662c-4b6f-b9e8-aea1cd4d2554",
   "metadata": {},
   "source": [
    "## RNNの構造定義と訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b47fa7",
   "metadata": {},
   "source": [
    "### ステップ7\n",
    "RNNの構造定義\n",
    "* 基本的なRNNを適用\n",
    "* RNN を LSTM や GRU に変更可能\n",
    "* num_lahyers オプションで RNNを複数層重ねることもできる\n",
    "* bidirectional オプションを用いることで双方向へのデータの伝播も可能（隠れ層のサイズの調整は必要）\n",
    "\n",
    "RNNの訓練方法\n",
    "* 最適化アルゴリズムは Adam を使用\n",
    "* 損失関数は平均二乗誤差（Mean Squared Error：MSE）を使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae596b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(L.LightningModule):\n",
    "    def __init__(self, input_size, rnn_hidden_size, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # ハイパーパラメータを保存\n",
    "        self.example_input_array = torch.zeros((1, sequence_length, 1)) # データ数1，シーケンス長 sequence_length，データ次元数 1\n",
    "\n",
    "        # RNN: 最も単純なRNN層を定義．系列データをずらしながら入力\n",
    "        #            ユニット数（層の出力の次元数）64 (変更可）のデータを次の層へ出力\n",
    "        #            活性化関数は tanh（ハイパボリックタンジェント：変更可）\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=rnn_hidden_size, nonlinearity='tanh',\n",
    "                          num_layers=1, bidirectional=False,\n",
    "                          batch_first=True)\n",
    "\n",
    "        # RNN層をLSTMにする場合\n",
    "        #self.rnn = nn.LSTM(input_size=input_size, hidden_size=rnn_hidden_size,\n",
    "        #                     batch_first=True)\n",
    "        # RNN層をGRUにする場合\n",
    "        #self.rnn = nn.GRU(input_size=input_size, hidden_size=rnn_hidden_size,\n",
    "        #                    batch_first=True)\n",
    "        #\n",
    "        # RNN層を複数にする場合は num_layers を増やす\n",
    "        #self.rnn = nn.RNN(input_size=input_size, hidden_size=rnn_hidden_size, nonlinearity='tanh',\n",
    "        #                    num_layers=2,\n",
    "        #                    batch_first=True)\n",
    "        # RNN層を双方向（前の時刻，後の時刻への伝播を両方含む）ものにする場合，bidirectional=Trueにする\n",
    "        # 両方向の hidden_size を足し合わせたものが 次のLayerに入力されるので、rnn_hidden_sizeを2で割っている\n",
    "        #self.rnn = nn.RNN(input_size=input_size, hidden_size=rnn_hidden_size // 2, nonlinearity='tanh',\n",
    "        #                    bidirectional=True,\n",
    "        #                    batch_first=True)\n",
    "        #        \n",
    "        # Layer: 全結合のニューロン層 ユニット数 1，活性化関数（出力関数） linear（変換なしでそのまま出力）\n",
    "        self.dense = nn.Linear(in_features=rnn_hidden_size, out_features=1)\n",
    "        # 双方向RNNの場合は in_features=rnn_hidden_size * 2 にする\n",
    "        #self.dense = nn.Linear(in_features=rnn_hidden_size * 2, out_features=1)\n",
    "\n",
    "        # 損失関数: 回帰問題なので平均二乗誤差（MSE）\n",
    "        self.criterion = nn.MSELoss()\n",
    "        # 学習率\n",
    "        self.lr = lr\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # RNN層への入力と実行\n",
    "        # rnn_out: 全タイムステップの出力 (batch_size, sequence_length, rnn_units)\n",
    "        # h_n: 最終層の最後の隠れ状態 (num_layers * num_directions, batch_size, rnn_units)\n",
    "        rnn_out, h_n = self.rnn(x)\n",
    "        # 最終時刻の出力を取り出す\n",
    "        out = rnn_out[:, -1, :]\n",
    "        # 最終時刻の出力を全結合層へ入力\n",
    "        out = self.dense(out) # 全結合層への入力\n",
    "        return out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # オプティマイザ:\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        # on_epoch=True でエポック終了時に平均値を記録\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # 訓練データのバッチを使って損失を計算\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        # on_epoch=True でエポック終了時に平均値を記録\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # 訓練データのバッチを使って損失を計算\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        # on_epoch=True でエポック終了時に平均値を記録\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "ModelSummary(RNN(input_size=1, rnn_hidden_size=64, lr=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25690b08",
   "metadata": {},
   "source": [
    "### ステップ8\n",
    "作成したモデルの訓練\n",
    "* エポック数 100（ただし損失の値に変化がなくなったら訓練を打ち切る EarlyStopping コールバックを利用）\n",
    "* バッチサイズ 100 （100コミットを使って予測するので，それに合わせている）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40a547",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100 # 最大エポック数\n",
    "batch_size = 100 # バッチサイズ\n",
    "patience = 20 # 訓練を打ち切る場合でも最低20エポックまでは訓練する\n",
    "learning_rate = 0.001 # 学習率\n",
    "\n",
    "# 1. データローダーとモデルのインスタンス化\n",
    "\n",
    "# 入力データ：training_seqs 気温の系列データ\n",
    "# 正解データ：training_targets 各系列に対する翌日の気温\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# バッチサイズ：32（このセルの先頭部分の batch_size）\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "model_rnn = RNN(input_size=1, rnn_hidden_size=64, lr=learning_rate)\n",
    "\n",
    "checkpoint_callback = L.callbacks.ModelCheckpoint(\n",
    "    save_last=True,\n",
    ")\n",
    "# erly stopping コールバックの設定（検証データの損失が改善しなくなったら学習を終了する）\n",
    "early_stopping_callback = L.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=patience,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "log_folder = Path(base_path, \"logs\")\n",
    "rnn_logger = L.loggers.CSVLogger(log_folder, name=\"rnn_\" + csv_file_name.stem)\n",
    "rnn_trainer = L.Trainer(\n",
    "    max_epochs=epochs, # エポック数：100（このセルの先頭部分の epochs）\n",
    "    logger=rnn_logger,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    deterministic=True,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "# モデルの訓練\n",
    "rnn_trainer.fit(model_rnn, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "checkpoint_path = checkpoint_callback.last_model_path\n",
    "stopped_epoch = rnn_trainer.early_stopping_callback.stopped_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3992db1e-d8c4-4ddd-8c9e-c2517cb913b7",
   "metadata": {},
   "source": [
    "### ステップ9\n",
    "訓練履歴の確認\n",
    "* 訓練中のエポック毎の損失値（平均二乗誤差）の推移を描画\n",
    "* ここでの損失は平均と標準偏差によってデータを標準化したものに対する値であることに注意\n",
    "* グラフ描画の関数を定義して適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef19cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_history(log_path):\n",
    "    log_file = Path(log_path, \"metrics.csv\")\n",
    "    history = pd.read_csv(log_file)\n",
    "    # 記録のタイミングにより欠損値があるため，欠損値のある行を削除\n",
    "    train_history = history.dropna(subset=['train_loss']).reset_index(drop=True) \n",
    "    val_history = history.dropna(subset=['val_loss']).reset_index(drop=True)\n",
    "    return train_history, val_history\n",
    "\n",
    "def show_loss_graphs(train_history, val_history, epochs, epoch_from = 0): # 訓練履歴（精度・損失）を描画する関数\n",
    "    training_losses = train_history[\"train_loss\"] # 訓練用データに対する損失\n",
    "    validation_losses = val_history[\"val_loss\"] # 評価用データに対する損失\n",
    "    \n",
    "    epochs_range = range(epoch_from, epochs + 1) # 1 から epochs までの描画範囲を指定\n",
    "    figure = plt.figure()\n",
    "    subplot = plt.subplot()\n",
    "    subplot.plot(epochs_range, training_losses, label = \"Training Loss\") # 訓練用データに対する損失のグラフ描画\n",
    "    subplot.plot(epochs_range, validation_losses, label = \"Validation Loss\") # 評価用データに対する損失のグラフ描画\n",
    "    subplot.set_xlabel(\"epochs\")\n",
    "    subplot.legend()\n",
    "    # plt.ylim(0, 0.2) # 複数のモデルに対して損失値のグラフを比べたい場合，この行のコメントを外し，固定の上限値，下限値を指定する\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.show()\n",
    "\n",
    "log_path = Path(log_folder, \"rnn_\" + csv_file_name.stem, f\"version_{rnn_trainer.logger.version}\")\n",
    "rnn_train_history, rnn_val_history = load_history(log_path)\n",
    "show_loss_graphs(rnn_train_history, rnn_val_history, stopped_epoch) # 損失値のグラフを描画"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49025fb-93d0-4da6-bbac-9b94018995ad",
   "metadata": {},
   "source": [
    "## 予測タスクの実行\n",
    "テスト用データを与えてコミット当たり変更行数を予測する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f9317-3185-423d-bcd6-2c9a9accb0ba",
   "metadata": {},
   "source": [
    "## ステップ10\n",
    "* 訓練したモデルに2021/1/1から2022/12/31までのデータを与えて予測\n",
    "* 出力されるのは平均と標準偏差を用いて変換したものなので，逆変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31200ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 系列データを与えて予測\n",
    "test_dataloader = DataLoader(\n",
    "    test_seqs,\n",
    "    batch_size=len(test_seqs),\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "predictions = rnn_trainer.predict(model_rnn, dataloaders=test_dataloader)\n",
    "predictions = torch.cat(predictions, dim=0).numpy() # バッチごとの要素のリストを結合して1つのTensorにする\n",
    "\n",
    "# 予測結果とテスト用の正解データを標準化逆変換\n",
    "predictions_real = scaler.inverse_transform(predictions)\n",
    "targets_real = scaler.inverse_transform(test_targets)\n",
    "\n",
    "# 誤差に関する値を表示（平方根平均二乗誤差，平均絶対誤差，平均絶対パーセント誤差）\n",
    "print(\"平方根平均二乗誤差（RMSE）= {}\".format(np.sqrt(mean_squared_error(targets_real, predictions_real))))\n",
    "print(\"平均絶対誤差（MAE）= {}\".format(mean_absolute_error(targets_real, predictions_real)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d08abb-7f44-4ed9-8c30-cc5bb493038a",
   "metadata": {},
   "source": [
    "### ステップ11\n",
    "予測結果のグラフ描画\n",
    "* 2022/1/1から2022/12/31までの結果\n",
    "* 2022/1から2022/12までの1ヶ月ずつの予測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea212de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(predictions, targets, title = \"\"): # 予測結果と正解のグラフを表示する関数\n",
    "    plt.plot(predictions, label = \"predicted changed lines.\")\n",
    "    plt.plot(targets, label = \"correct (real) changed lines.\")\n",
    "    plt.ylim(0, 1600.0) # グラフの比較ができるように行数の範囲を固定（0行～1600行）\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# グラフ描画の開始日から終了日を指定して描画する関数を定義\n",
    "# 開始日と終了日は 2022/1/1から2022/12/31までの間で指定（日付の間のエラーチェックなどはしていないので注意）\n",
    "def show_result_with_start_and_end(start_date, end_date, offset, predictions, targets):\n",
    "    # 予測結果・正解データの最初の日\n",
    "    offset_date = date(year = 2021, month = 1, day = 1) + timedelta(days = offset)\n",
    "    # 描画開始位置の計算\n",
    "    diff_start = (start_date - offset_date).days \n",
    "    # 描画終了位置の計算\n",
    "    diff_end = (end_date - offset_date).days + 1\n",
    "    # 予測値と実際の値（正解）のグラフ描画\n",
    "    show_result(predictions[diff_start : diff_end],\n",
    "                targets[diff_start : diff_end],\n",
    "                title = \"{}\".format(start_date) + \" - {}\".format(end_date))\n",
    "\n",
    "# 2022/1/1 から 2022/12/31 までの予測結果と実際の値（正解）を描画\n",
    "# 以下，テストデータは 2021/1/1から2022/12/31 までの範囲でしか与えられておらず，\n",
    "# 予測結果や正解（実際の値）はその範囲に対するもののみであることに注意，\n",
    "# 範囲外の日付（年・月・日）を指定するとうまく動かない\n",
    "show_result_with_start_and_end(date(year = 2022, month = 1, day = 1), \n",
    "                               date(year = 2022, month = 12, day = 31), \n",
    "                               offset = sequence_length, predictions = predictions_real, targets = targets_real)\n",
    "\n",
    "# 2022/1 から一か月ごとに予測結果と実際の値（正解）を描画\n",
    "for i in range(1, 13):\n",
    "    if i == 12: # 12月だけ特別扱い\n",
    "        ey = 2023\n",
    "        em = 1\n",
    "    else:\n",
    "        ey = 2022\n",
    "        em = i + 1\n",
    "    # 各月の1日から最終日までのグラフを描画\n",
    "    show_result_with_start_and_end(date(year = 2022, month = i, day = 1),\n",
    "                                   date(year = ey, month = em, day = 1) - timedelta(days = 1),\n",
    "                                   offset = sequence_length, predictions = predictions_real, targets = targets_real)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
